{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a Simple Summarizer in Python from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/write-a-simple-summarizer-in-python-e9ca6138a08e\n",
    "\n",
    "Step 1. Read from source — Read the unabridged content from the source, a file in the case of this exercise.\n",
    "\n",
    "Step 2. Perform formatting and cleanup — Format and clean up our format so that it is free of extra white space or other issues.\n",
    "\n",
    "Step 3. Tokenize input — Take the input and break it up into its individual words.\n",
    "\n",
    "Step 4. Scoring — Score (count) the frequency of each word in the input and score sentences based on word score.\n",
    "\n",
    "Step 5. Selection — Choose the top N sentences based on their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            return file.read()\n",
    "    except IOError as e:\n",
    "        print(\"Fatal Error: File ({}) could not be located or is not readable.\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanitize_input(data):\n",
    "    replace = {\n",
    "        ord('\\f') : ' ',\n",
    "        ord('\\t') : ' ',\n",
    "        ord('\\n') : ' ',\n",
    "        ord('\\r') : None\n",
    "    }\n",
    "    return data.translate(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_content(content):\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    words = word_tokenize(content.lower())\n",
    "    \n",
    "    return [\n",
    "        sent_tokenize(content),\n",
    "        [word for word in words if word not in stop_words]    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_tokens(filtered_words, sentence_tokens):\n",
    "    word_freq = FreqDist(filtered_words)\n",
    "    ranking = defaultdict(int)\n",
    "    for i, sentence in enumerate(sentence_tokens):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                ranking[i] += word_freq[word]\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(ranks, sentences, length):\n",
    "    if int(length) > len(sentences): \n",
    "        print(\"Error, more sentences requested than available. Use --l (--length) flag to adjust.\")\n",
    "        exit()\n",
    "    indexes = nlargest(length, ranks, key=ranks.get)\n",
    "    final_sentences = [sentences[j] for j in sorted(indexes)]\n",
    "    return ' '.join(final_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like a bowling ball on a skating rink, the black geodesic sphere of the East Greenland Ice-Core Project's communal living space stands out against the endless white nothingness of the Greenland ice sheet.  But the real action at East GRIP is under the surface. Researchers are drilling through more than 2.5 kilometers of ice, down to the bedrock below. The ice is sliding fast - for a glacier - toward the sea. Scientists here want to know why. The answer may hold clues to the future of the world's coastal cities.  Greenland is melting. As it melts, it adds roughly 1 millimeter of water per year to global sea levels. And the pace of melting is quickening.  If all the ice covering the world's largest island were to thaw, sea levels would rise roughly 6 meters. Scientists don't know how fast, or how likely, that is to happen. East GRIP is looking for evidence to inform both those questions.  The answers are a matter of growing urgency. The seas are rising faster. And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further.  The Arctic is warming twice as fast as the rest of the planet. Scientists studying the rapid changes gather in the small Greenland town of Kangerlussuaq, a former U.S. military base built during World War II. Through the Cold War, this outpost supplied remote radar sites watching for a nuclear attack coming over the pole.  These days, military transport planes fly scientists and their equipment across 1,000 kilometers of Arctic ice to East GRIP. They make research possible here and at other far-flung scientific outposts on the vast Greenland ice sheet.  Departing from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change. \n",
      "\n",
      "[\"Like a bowling ball on a skating rink, the black geodesic sphere of the East Greenland Ice-Core Project's communal living space stands out against the endless white nothingness of the Greenland ice sheet.\", 'But the real action at East GRIP is under the surface.', 'Researchers are drilling through more than 2.5 kilometers of ice, down to the bedrock below.', 'The ice is sliding fast - for a glacier - toward the sea.', 'Scientists here want to know why.', \"The answer may hold clues to the future of the world's coastal cities.\", 'Greenland is melting.', 'As it melts, it adds roughly 1 millimeter of water per year to global sea levels.', 'And the pace of melting is quickening.', \"If all the ice covering the world's largest island were to thaw, sea levels would rise roughly 6 meters.\", \"Scientists don't know how fast, or how likely, that is to happen.\", 'East GRIP is looking for evidence to inform both those questions.', 'The answers are a matter of growing urgency.', 'The seas are rising faster.', \"And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further.\", 'The Arctic is warming twice as fast as the rest of the planet.', 'Scientists studying the rapid changes gather in the small Greenland town of Kangerlussuaq, a former U.S. military base built during World War II.', 'Through the Cold War, this outpost supplied remote radar sites watching for a nuclear attack coming over the pole.', 'These days, military transport planes fly scientists and their equipment across 1,000 kilometers of Arctic ice to East GRIP.', 'They make research possible here and at other far-flung scientific outposts on the vast Greenland ice sheet.', 'Departing from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change.'] \n",
      "\n",
      "['like', 'bowling', 'ball', 'skating', 'rink', 'black', 'geodesic', 'sphere', 'east', 'greenland', 'ice-core', 'project', \"'s\", 'communal', 'living', 'space', 'stands', 'endless', 'white', 'nothingness', 'greenland', 'ice', 'sheet', 'real', 'action', 'east', 'grip', 'surface', 'researchers', 'drilling', '2.5', 'kilometers', 'ice', 'bedrock', 'ice', 'sliding', 'fast', 'glacier', 'toward', 'sea', 'scientists', 'want', 'know', 'answer', 'may', 'hold', 'clues', 'future', 'world', \"'s\", 'coastal', 'cities', 'greenland', 'melting', 'melts', 'adds', 'roughly', '1', 'millimeter', 'water', 'per', 'year', 'global', 'sea', 'levels', 'pace', 'melting', 'quickening', 'ice', 'covering', 'world', \"'s\", 'largest', 'island', 'thaw', 'sea', 'levels', 'would', 'rise', 'roughly', '6', 'meters', 'scientists', \"n't\", 'know', 'fast', 'likely', 'happen', 'east', 'grip', 'looking', 'evidence', 'inform', 'questions', 'answers', 'matter', 'growing', 'urgency', 'seas', 'rising', 'faster', 'processes', 'work', 'greenland', \"'s\", 'glaciers', 'top', 'world', 'could', 'send', 'vast', 'sections', 'antarctica', \"'s\", 'ice', 'sheet', 'sea', 'well', 'raising', 'ocean', 'levels', 'even', 'arctic', 'warming', 'twice', 'fast', 'rest', 'planet', 'scientists', 'studying', 'rapid', 'changes', 'gather', 'small', 'greenland', 'town', 'kangerlussuaq', 'former', 'u.s.', 'military', 'base', 'built', 'world', 'war', 'ii', 'cold', 'war', 'outpost', 'supplied', 'remote', 'radar', 'sites', 'watching', 'nuclear', 'attack', 'coming', 'pole', 'days', 'military', 'transport', 'planes', 'fly', 'scientists', 'equipment', 'across', '1,000', 'kilometers', 'arctic', 'ice', 'east', 'grip', 'make', 'research', 'possible', 'far-flung', 'scientific', 'outposts', 'vast', 'greenland', 'ice', 'sheet', 'departing', 'kangerlussuaq', 'voa', 'visited', 'east', 'grip', 'remote', 'corners', 'greenland', '109th', 'airlift', 'wing', 'u.s.', 'air', 'national', 'guard', 'firsthand', 'look', 'science', 'action', 'leading', 'edge', 'climate', 'change'] \n",
      "\n",
      "defaultdict(<class 'int'>, {0: 51, 1: 13, 2: 13, 3: 17, 4: 7, 5: 16, 6: 9, 7: 17, 8: 4, 9: 33, 10: 12, 11: 13, 12: 4, 13: 3, 14: 52, 15: 9, 16: 33, 17: 14, 18: 33, 19: 25, 20: 41}) \n",
      "\n",
      "Like a bowling ball on a skating rink, the black geodesic sphere of the East Greenland Ice-Core Project's communal living space stands out against the endless white nothingness of the Greenland ice sheet. And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further. Departing from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change.\n"
     ]
    }
   ],
   "source": [
    "content = read_file(\"Greenland-Melting-Full.txt\")\n",
    "content = sanitize_input(content)\n",
    "print(content,\"\\n\")\n",
    "sentence_tokens, word_tokens = tokenize_content(content)  \n",
    "print(sentence_tokens,\"\\n\")\n",
    "print(word_tokens,\"\\n\")\n",
    "    \n",
    "sentence_ranks = score_tokens(word_tokens, sentence_tokens)\n",
    "print(sentence_ranks,\"\\n\")\n",
    "\n",
    "print(summarize(sentence_ranks, sentence_tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization using Gensim (uses TextRank based summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[38]\\n In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[39] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[40][41]\\n Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of ANNs\\' computational cost and a lack of understanding of how the brain wires its biological networks.\\n Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[42][43][44] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[45] Key difficulties have been analyzed, including gradient diminishing[40] and weak temporal correlation structure in neural predictive models.[46][47] Additional difficulties were the lack of training data and limited computing power.\\n Most speech recognition researchers moved away from neural nets to pursue generative modeling.\n",
      "\n",
      "Keywords:\n",
      "learns\n",
      "learn\n",
      "learned\n",
      "deep learning\n",
      "layer\n",
      "layers\n",
      "layered\n",
      "image\n",
      "images\n",
      "models\n",
      "model\n",
      "modeling\n",
      "recognition\n",
      "networks\n",
      "network\n",
      "trained\n",
      "training\n",
      "train\n",
      "trains\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "#http://ai.intelligentonlinetools.com/ml/text-summarization/\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    " \n",
    "def get_only_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    return soup.title.text, text    \n",
    " \n",
    "url=\"https://en.wikipedia.org/wiki/Deep_learning\"\n",
    "text = get_only_text(url)\n",
    " \n",
    "print ('Summary:')   \n",
    "print (summarize(str(text), ratio=0.01))\n",
    " \n",
    "print ('\\nKeywords:')\n",
    "# higher ratio => more keywords\n",
    "print (keywords(str(text), ratio=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization using sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--LsaSummarizer--\n",
      "[40][41]\\n Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of ANNs\\' computational cost and a lack of understanding of how the brain wires its biological networks.\\n Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.\n",
      "[127] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\\n Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks.\n",
      "[132] Deep neural architectures provide the best results for constituency parsing,[133] sentiment analysis,[134] information retrieval,[135][136] spoken language understanding,[137] machine translation,[103][138] contextual entity linking,[138] writing style recognition,[139] Text classification and others.\n",
      "\n",
      "--LuhnSummarizer--\n",
      "[46][47] Additional difficulties were the lack of training data and limited computing power.\\n Most speech recognition researchers moved away from neural nets to pursue generative modeling.\n",
      "[84]\\n In 2012, a team led by Dahl won the \"Merck Molecular Activity Challenge\" using multi-task deep neural networks to predict the biomolecular target of one drug.\n",
      "[132] Deep neural architectures provide the best results for constituency parsing,[133] sentiment analysis,[134] information retrieval,[135][136] spoken language understanding,[137] machine translation,[103][138] contextual entity linking,[138] writing style recognition,[139] Text classification and others.\n",
      "\n",
      "--EdmundsonSummarizer--\n",
      "('Deep learning - Wikipedia', '\\n Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms.\n",
      "[7][8][9] Deep learning is a class of machine learning algorithms that:[10](pp199–200)\\n Most modern deep learning models are based on an artificial neural network, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\n",
      "[2] No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth > 2.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    " \n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer   #found this is the best as \n",
    "# it is picking from beginning also while other skip\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 3\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    url=\"https://en.wikipedia.org/wiki/Deep_learning\"\n",
    "    text = get_only_text(url)\n",
    "    parser = PlaintextParser(text, Tokenizer(LANGUAGE))\n",
    "    # or for plain text files\n",
    "    # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    " \n",
    "    print (\"--LsaSummarizer--\")\n",
    "    summarizer = LsaSummarizer(Stemmer(LANGUAGE))#Latent Semantic Analysis\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "         \n",
    "    print (\"\\n--LuhnSummarizer--\")     \n",
    "    summarizer = LuhnSummarizer(Stemmer(LANGUAGE)) #Word freq method\n",
    "    summarizer.stop_words = (\"I\", \"am\", \"the\", \"you\", \"are\", \"me\", \"is\", \"than\", \"that\", \"this\",)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "         \n",
    "    print (\"\\n--EdmundsonSummarizer--\")     \n",
    "    summarizer = EdmundsonSummarizer() #cue phrase method\n",
    "    words = (\"deep\", \"learning\", \"neural\" )\n",
    "    summarizer.bonus_words = words\n",
    "    words = (\"another\", \"and\", \"some\", \"next\",)\n",
    "    summarizer.stigma_words = words\n",
    "    words = (\"another\", \"and\", \"some\", \"next\",)\n",
    "    summarizer.null_words = words\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LexRank using sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His only connection to the rest of the world is a little girl, So-mee, who lives nearby.\n",
      "At the same time, Hyo-jeongs body, with her organs harvested, is discovered in the back of the car used by Tae-sik when he made the delivery, and Tae-sik realizes that So-mis life may also be in danger.\n",
      "Realizing this, the Narcotics head contacted a weakened Tae-sik after his encounter with a highly trained assassin who backs up the brothers.\n",
      "Now with the knowledge that So-mee is being used by an ANT organization to secretly smuggle drugs and - in the future might be killed for organ harvesting, Tae-sik goes on a mad killing.hunting spree to locate and save So-mee, who is his only connection to a caring world.\n",
      "Tae-sik went mad when a container that holds harvested eyes was rolled towards him, believing to be So-mees.\n"
     ]
    }
   ],
   "source": [
    "text='''Operating a pawn shop in a small neighborhood, Cha Tae-sik now leads a quiet life. His only connection to the rest of the world is a little girl, So-mee, who lives nearby. A heroin addict and So-mis mother, Hyo-jeong, smuggles drugs from a drug trafficking organization and entrusts Tae-sik with the product without his knowledge. When the traffickers find out about this they kidnap both Hyo-jeong and So-mi. The gang sends a number of thugs to Tae-sik's pawn shop to retrieve the stolen drugs, but is easily overpowered by Tae-sik, making his identity ambiguous. However, upon learning that the gang now has in their possession both Hyo-jeong and So-mi, Tae-sik gives the beaten gang members what they are looking for.\n",
    "Realizing that Tae-sik may serve better as a mule than their former thug, the brothers that lead the gang Man-sik and Jong-sik promise to release Hyo-jeong and So-mi under the condition that Tae-sik make a delivery for them. Tae-sik makes the decision to face the outside world in order to rescue So-mi. However, the delivery was part of a larger plot to eliminate a drug ring superior, Mr. Oh, and Tae-sik is arrested. At the same time, Hyo-jeongs body, with her organs harvested, is discovered in the back of the car used by Tae-sik when he made the delivery, and Tae-sik realizes that So-mis life may also be in danger. He fights off half a dozen detectives and escapes from the police station. During his escape, the police are bewildered at Tae-sik's display of power, combat techniques and agility, and further investigates his bio and finds out that he was once a black operation agent for the Korean government with numerous commendations, but dropped out from the agency after witnessing his pregnant wife being murdered in front of his eyes in connection with him by being hit by a truck while inside their car. The incident nearly drove him mad, hence he went into hiding.\n",
    "Realizing this, the Narcotics head contacted a weakened Tae-sik after his encounter with a highly trained assassin who backs up the brothers. Now with the knowledge that So-mee is being used by an ANT organization to secretly smuggle drugs and - in the future might be killed for organ harvesting, Tae-sik goes on a mad killing.hunting spree to locate and save So-mee, who is his only connection to a caring world.\n",
    "A gore battle ensues, from the ANT/Drug manufacturing location where he was able to free the remaining children and kill off the younger of the brothers, to their posh condo unit where the rest of the killing continues. Tae-sik went mad when a container that holds harvested eyes was rolled towards him, believing to be So-mees. A final stand-off with the assassin ensues, with Tae-sik winning the fight.\n",
    "After killing off the last hoodlum in the parking lot, Tae-sik was about to resign to his fate by shooting himself when a scared and dirty So-mee emerged from the darkness. She was saved by the assassin who took to her kindly and killed instead the surgeon accomplice - his eyes were in the container that Tae-sik saw.\n",
    "A police escort in the end sees Tae-sik and So-mee together in the back of the detectives car. While she sleeps. Tae-sik asks if they can be dropped off at the small convenience store - a surprise for So-mee. The owner got a shock upon seeing all the police lined up around them, and with the words: You really messed it up big time.\n",
    "Tae-sik pulls up a fancy backpack with a star, and fills it up with fancy school stuff, much to her delight.\n",
    "Tae-sik asks if he can hug her, and as they embrace, he could not stop the tears as they fell from his battered bandaged face.'''\n",
    "\n",
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser #We're choosing a plaintext parser here, other parsers available for HTML etc.\n",
    "from sumy.nlp.tokenizers import Tokenizer \n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer #We're choosing Lexrank, other algorithms are also built in\n",
    "\n",
    "#file = \"plain_text.txt\" #name of the plain-text file\n",
    "#parser = PlaintextParser.from_file(file, Tokenizer(\"english\"))\n",
    "\n",
    "parser = PlaintextParser(text, Tokenizer(\"english\"))\n",
    "summarizer = LexRankSummarizer()\n",
    "\n",
    "summary = summarizer(parser.document, 5) #Summarize the document with 5 sentences\n",
    "\n",
    "for sentence in summary:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization using PyTeaser (works with Python 2 only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries are created by ranking sentences in a news article according to how relevant they are to the entire text. \n",
    "\n",
    "The top 5 sentences are used to form a \"summary\". Each sentence is ranked by using four criteria:\n",
    "\n",
    "1. Relevance to the title\n",
    "\n",
    "2. Relevance to keywords in the article\n",
    "\n",
    "3. Position of the sentence\n",
    "\n",
    "4. Length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"Twitter's move is the latest response from U.S. Internet firms following disclosures by former spy agency contractor Edward Snowden about widespread, classified U.S. government surveillance programs.\", u'\"Since then, it has become clearer and clearer how important that step was to protecting our users\\' privacy.\"', u'\"A year and a half ago, Twitter was first served completely over HTTPS,\" the company said in a blog posting.', u'The online messaging service, which began scrambling communications in 2011 using traditional HTTPS encryption, said on Friday it has added an advanced layer of protection for HTTPS known as \"forward secrecy.\"', u'(Reuters) - Twitter Inc said it has implemented a security technology that makes it harder to spy on its users and called on other Internet firms to do the same, as Web providers look to thwart spying by government intelligence agencies.']\n"
     ]
    }
   ],
   "source": [
    "from pyteaser import SummarizeUrl\n",
    "url = 'http://www.huffingtonpost.com/2013/11/22/twitter-forward-secrecy_n_4326599.html'\n",
    "summaries = SummarizeUrl(url)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank using summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/summanlp/textrank\n",
    "text = \"Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. As the problem of information overload has grown, and as the quantity of data has increased, so has interest in automatic summarization. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax. An example of the use of summarization technology is search engines such as Google. Document summarization is another.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\n"
     ]
    }
   ],
   "source": [
    "from summa import summarizer\n",
    "print(summarizer.summarize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document\n",
      "automatic summarization\n",
      "technologies\n",
      "technology\n"
     ]
    }
   ],
   "source": [
    "#extract keywords\n",
    "from summa import keywords\n",
    "print(keywords.keywords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\\nAn example of the use of summarization technology is search engines such as Google.\\nDocument summarization is another.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from summa.summarizer import summarize\n",
    "summarize(text, ratio=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\\nDocument summarization is another.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\\nAs the problem of information overload has grown, and as the quantity of data has increased, so has interest in automatic summarization.\\nAn example of the use of summarization technology is search engines such as Google.\\nDocument summarization is another.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\\nAs the problem of information overload has grown, and as the quantity of data has increased, so has interest in automatic summarization.\\nTechnologies that can make a coherent summary take into account variables such as length, writing style and syntax.\\nAn example of the use of summarization technology is search engines such as Google.\\nDocument summarization is another.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text, words=20)\n",
    "print()\n",
    "summarize(text, words=40)\n",
    "print()\n",
    "summarize(text, words=60)\n",
    "print()\n",
    "summarize(text, words=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here are some other summarizers:\n",
    "\n",
    " - https://github.com/thavelick/summarize/ - Python, TF (very simple)\n",
    " - Reduction - Python, TextRank (simple)\n",
    " - Open Text Summarizer - C, TF without normalization\n",
    " - Simple program that summarize text - Python, TF without normalization\n",
    " - Intro to Computational Linguistics - Java, LexRank\n",
    " - Sumtract: Second project for UW LING 572 - Python\n",
    " - TextTeaser - Scala\n",
    " - PyTeaser - TextTeaser port in Python\n",
    " - Automatic Document Summarizer - Java, Bipartite HITS (no sources)\n",
    " - Pythia - Python, LexRank & Centroid\n",
    " - SWING - Ruby\n",
    " - Topic Networks - R, topic models & bipartite graphs\n",
    " - Almus: Automatic Text Summarizer - Java, LSA (without source code)\n",
    " - Musutelsa - Java, LSA (always freezes)\n",
    " - http://mff.bajecni.cz/index.php - C++\n",
    " - MEAD - Perl, various methods + evaluation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
